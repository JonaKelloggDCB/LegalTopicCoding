---
title: "Legislative Topic Coding and Textual Structure"
author: "Jonathan Kellogg"
date: "2025-08-29"
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning = FALSE
)
```


# Introduction

This RMarkdown document accompanies the thesis *“Frankenstein’s Bill: The Political Utility of Disjointed Legislation.”* It provides a transparent and reproducible account of the data construction, preprocessing, segmentation, and classification procedures used in the empirical analysis.

The aim of this document is methodological rather than theoretical. It details how legislative texts are harmonised across jurisdictions and how section-level measures of textual prominence, divergence, and topical content are derived. All analytical choices are documented to facilitate replication.

---

# Data Sources

The legislative corpus draws on three established sources:

1. **United States** legislative texts from the Billsum dataset (Kornilova & Eidelman 2019), which provides bill-level texts and metadata for congressional legislation.
2. **European Union** legal acts from Eur-Lex (Laurer & Borrett 2020), restricted to decisions that are in force at the time of data collection.
3. **French** legislative texts provided by the Harvard Library Innovation Lab (2024), covering statutory articles with English translations.

Each source differs substantially in institutional structure and textual conventions. The preprocessing steps described below are designed to impose a minimal but consistent structure across jurisdictions.

--- 


# Package Management and Reproducibility

All required packages are loaded explicitly at the beginning of the script. Dependency management is handled using the `pacman` package to ensure that the computational environment can be reproduced on a clean system.

Messages and warnings are suppressed during knitting to improve readability, while all code remains visible for transparency.

```{r}
# Install pacman if needed

if (!requireNamespace("pacman", quietly = TRUE)) {
install.packages("pacman")
}

# Load required packages

pacman::p_load(
tidyverse,
jsonlite,
data.table,
stringr,
purrr,
tibble,
rvest,
httr2,
mall,
ollamar,
ellmer,
reticulate, 
tokenizers
)



```

---

# File Structure and Paths

All file paths are defined relative to a single project directory.

```{r}
data_dir <- "/Users/jonathankellogg/Desktop/LegalTopicCoding"

paths <- list(
  us_train = file.path(data_dir, "billsum_v4_1/us_train_data_final_OFFICIAL.jsonl"),
  us_test  = file.path(data_dir, "billsum_v4_1/us_test_data_final_OFFICIAL.jsonl"),
  eurlex   = file.path(data_dir, "EurLex_all.csv"),
  france   = file.path(data_dir, "cold-french-law.csv")
)
```

---

# United States Legislative Texts

## Data Loading

Training and test files from the Billsum dataset are loaded separately and then merged into a single dataset. The distinction between training and test sets is irrelevant for the purposes of this analysis, which treats all legislative texts uniformly.

```{r}
us_train <- stream_in(file(paths$us_train), flatten = TRUE) |> as.data.frame()
us_test  <- stream_in(file(paths$us_test),  flatten = TRUE) |> as.data.frame()

us_all <- bind_rows(us_train, us_test)
```

## Variable Selection and Cleaning

Only variables required for the analysis are retained. Congressional session numbers are extracted from bill identifiers and converted into calendar years using the standard two-year congressional cycle.

Legislative text is normalised by removing line breaks and excess whitespace.

```{r}
us_all <- us_all |>
  select(bill_id, text, title, text_len) |>
  mutate(
    country  = "USA",
    congress = as.integer(substr(bill_id, 1, 3)),
    year     = 1789 + 2 * (congress - 1),
    text     = text |>
      str_replace_all("\n", " ") |>
      str_squish()
  )

```

## Political Control Variables

Party control of the House of Representatives, Senate, and Presidency is merged at the congressional level. These variables are included for use in subsequent empirical analysis.

```{r}
party_control <- tibble(
  congress = 103:115,
  house = c(
    "Democratic","Republican","Republican","Republican",
    "Republican","Republican","Republican","Democratic",
    "Democratic","Republican","Democratic","Republican",
    "Republican"
  ),
  senate = c(
    "Democratic","Republican","Republican","Republican",
    "Republican","Republican","Republican","Democratic",
    "Democratic","Democratic","Democratic","Republican",
    "Republican"
  ),
  president_party = c(
    "Democratic","Democratic","Democratic","Democratic",
    "Republican","Republican","Republican","Republican",
    "Democratic","Democratic","Democratic","Democratic",
    "Republican"
  )
)

us_all <- us_all |>
  left_join(party_control, by = "congress")

```

## Section-Level Segmentation

Bills are segmented into sections using regular expressions that identify numbered provisions and article headings. Each section is treated as a distinct textual unit.

```{r}
us_all <- us_all |>
  mutate(
    section = str_split(
      text,
      "(?=\\(\\d+\\))|(?=Article\\s+\\d+)"
    )
  ) |>
  unnest(section) |>
  mutate(section = str_trim(section)) |>
  filter(section != "")

```

---

# European Union Legislative Texts

EU legal acts are drawn from Eur-Lex and restricted to decisions that are in force. Texts are segmented using the same procedure applied to U.S. legislation to maximise comparability.

```{r}
eu_raw <- read.csv(paths$eurlex)

eu <- eu_raw |>
  filter(
    Act_type == "Decision",
    Status   == "In Force"
  ) |>
  select(
    CELEX,
    Act_name,
    act_raw_text
  ) |>
  rename(
    bill_id = CELEX,
    title   = Act_name,
    text    = act_raw_text
  ) |>
  mutate(country = "EU")

eu <- eu |>
  mutate(
    section = str_split(
      text,
      "(?=\\(\\d+\\))|(?=Article\\s+\\d+)"
    )
  ) |>
  unnest(section) |>
  mutate(section = str_trim(section)) |>
  filter(section != "")

```

---

# French Legislative Text

French legislative texts are provided at the article level and do not include a pre-constructed variable containing the full text of each law. Each observation therefore corresponds to an individual article rather than a complete legislative act.

To ensure comparability with the United States and European Union datasets, a bill-level text variable is constructed by concatenating all article texts belonging to the same legislative act. Articles are grouped using a unique bill identifier (`bill_id`), which prevents unintended aggregation across distinct laws that may share similar or identical titles.

The resulting dataset retains article-level observations while attaching a bill-level text variable to each row. This structure allows section-level measures—such as textual prominence and textual divergence—to be computed in a manner consistent with other jurisdictions, while avoiding assumptions about the substantive ordering of articles. Since the measures employed rely on relative textual content rather than sequence, article ordering is not imposed during concatenation.

```{r}
fr_raw <- read.csv(paths$france)

# ---- 1. Keep section-level observations ----
fr_sections <- fr_raw |>
  select(
    article_num,
    texte_titre_en,
    article_contenu_markdown_en
  ) |>
  rename(
    bill_id = article_num,
    title   = texte_titre_en,
    section = article_contenu_markdown_en
  ) |>
  mutate(
    section = section |>
      stringr::str_replace_all("\n", " ") |>
      stringr::str_squish(),
    country = "France"
  )

# ---- 2. Construct bill-level text by collapsing sections within bill_id ----
fr_texts <- fr_sections |>
  group_by(bill_id) |>
  summarise(
    text = paste(section, collapse = " "),
    .groups = "drop"
  )

# ---- 3. Reattach bill-level text to section-level data ----
fr <- fr_sections |>
  left_join(fr_texts, by = "bill_id")
```

---
# Dataset Integration

All jurisdiction-specific datasets are merged into a single harmonised dataset and written to disk for downstream analysis.

```{r}
df_all <- bind_rows(us_all, eu, fr)

write.csv(
  df_all,
  file.path(data_dir, "Total.csv"),
  row.names = FALSE
)

```

### Temporary Download

Due to the computational strain of this program, it is worth downloading the dataframe here to avoid problems later on. 

```{r}
download_path <- file.path(
  path.expand("~/Downloads"),
  "df_all_temp2.csv"
)

write.csv(
  df_all,
  download_path,
  row.names = FALSE
)
```

---

# Textual Prominence and Hiddenness

Textual prominence is operationalised as the proportion of a bill’s total sentences contained within a given section. Hiddenness is defined as the inverse of this measure.


```{r}

#textual prominence

df_all <- df_all %>%
  mutate(
    section_sentences = lengths(tokenize_sentences(section)),
    text_sentences    = lengths(tokenize_sentences(text)),
    prominence        = if_else(
      text_sentences > 0,
      section_sentences / text_sentences,
      NA_real_
    ),
    hiddenness_textual = 1 - prominence
  )


```

## Semantic divergence
calculate the semantic divergence of the title using a TF_IDF distance approach.
```{r}
library(text2vec)
library(Matrix)

stopifnot("section" %in% names(df_all), "title" %in% names(df_all))

# Build corpus: section then title (row-aligned)
corpus <- c(df_all$section, df_all$title)

it <- itoken(corpus, tokenizer = word_tokenizer, progressbar = TRUE)

VOCAB_TERM_MAX <- 3000
vocab <- create_vocabulary(it, vocab_term_max = VOCAB_TERM_MAX)

if (nrow(vocab) == 0) {
  stop("Vocabulary is empty after tokenization. Check that `section`/`title` contain text.")
}

vectorizer <- vocab_vectorizer(vocab)
dtm <- create_dtm(it, vectorizer)

tfidf <- TfIdf$new()
tfidf_mat <- tfidf$fit_transform(dtm)

# Ensure sparse matrix class stays 2D
tfidf_mat <- as(tfidf_mat, "dgCMatrix")

n <- nrow(df_all)

sec_mat   <- tfidf_mat[1:n, , drop = FALSE]
title_mat <- tfidf_mat[(n + 1):(2 * n), , drop = FALSE]

# Guard: if no columns, cannot compute distances
if (ncol(sec_mat) == 0) {
  stop("DTM/TF-IDF has 0 columns (no terms survived). Try increasing VOCAB_TERM_MAX or check text content.")
}

# Vectorised cosine distance using Matrix::rowSums (works for sparse matrices)
sec_len   <- sqrt(Matrix::rowSums(sec_mat * sec_mat))
title_len <- sqrt(Matrix::rowSums(title_mat * title_mat))

valid <- (sec_len > 0) & (title_len > 0)

df_all$divergence_vs_title <- NA_real_
df_all$divergence_vs_title[valid] <-
  1 - Matrix::rowSums(
    (sec_mat[valid, , drop = FALSE] / sec_len[valid]) *
    (title_mat[valid, , drop = FALSE] / title_len[valid])
  )
```

# Run independently for France

Due to difference in column names

```{r}
library(dplyr)
library(text2vec)
library(Matrix)

# --- Filter to France ---
df_france <- df_all %>%
  filter(country == "France")

# --- Guards ---
stopifnot(
  "text"  %in% names(df_france),
  "title" %in% names(df_france)
)

# --- Build corpus: text then title (row-aligned) ---
corpus <- c(df_france$text, df_france$title)

it <- itoken(
  corpus,
  tokenizer = word_tokenizer,
  progressbar = TRUE
)

VOCAB_TERM_MAX <- 3000
vocab <- create_vocabulary(it, vocab_term_max = VOCAB_TERM_MAX)

if (nrow(vocab) == 0) {
  stop("Vocabulary is empty after tokenization. Check that `text`/`title` contain text.")
}

vectorizer <- vocab_vectorizer(vocab)
dtm <- create_dtm(it, vectorizer)

tfidf <- TfIdf$new()
tfidf_mat <- tfidf$fit_transform(dtm)

# Ensure sparse matrix stays 2D
tfidf_mat <- as(tfidf_mat, "dgCMatrix")

n <- nrow(df_france)

text_mat  <- tfidf_mat[1:n, , drop = FALSE]
title_mat <- tfidf_mat[(n + 1):(2 * n), , drop = FALSE]

# Guard: if no columns, cannot compute distances
if (ncol(text_mat) == 0) {
  stop("DTM/TF-IDF has 0 columns (no terms survived). Check text content or vocab size.")
}

# --- Vectorised cosine distance ---
text_len  <- sqrt(Matrix::rowSums(text_mat  * text_mat))
title_len <- sqrt(Matrix::rowSums(title_mat * title_mat))

valid <- (text_len > 0) & (title_len > 0)

df_france$divergence_vs_title <- NA_real_
df_france$divergence_vs_title[valid] <-
  1 - Matrix::rowSums(
    (text_mat[valid, , drop = FALSE]  / text_len[valid]) *
    (title_mat[valid, , drop = FALSE] / title_len[valid])
  )

# --- Merge back into df_all ---
df_all$divergence_vs_title[df_all$country == "France"] <-
  df_france$divergence_vs_title



```

```{r}

# Validity regressions (individual + combined) 
# Outcome: TF-IDF cosine divergence (section vs title)
# Predictors tested:
#   1) div_text_tfidf alone
#   2) jsd_text alone
#   3) both together


library(dplyr)
library(text2vec)
library(Matrix)
library(stargazer)

set.seed(123)
VALID_N <- 1000
VOCAB_TERM_MAX <- 3000

stopifnot(all(c("section","text","title") %in% names(df_all)))

# -------------------------
# Sample validation subset
# -------------------------
df_ok <- df_all %>%
  filter(!is.na(section), !is.na(text), !is.na(title))

N_take <- min(VALID_N, nrow(df_ok))
idx <- sample.int(nrow(df_ok), size = N_take)
df_val <- df_ok[idx, ]
n <- nrow(df_val)

# -------------------------
# Build shared TF-IDF space (section + text + title)
# -------------------------
corpus <- c(df_val$section, df_val$text, df_val$title)
it <- itoken(corpus, tokenizer = word_tokenizer, progressbar = TRUE)

vocab <- create_vocabulary(it, vocab_term_max = VOCAB_TERM_MAX)
if (nrow(vocab) == 0) stop("Vocabulary is empty. Check your text columns.")
vectorizer <- vocab_vectorizer(vocab)

dtm <- create_dtm(it, vectorizer)
tfidf <- TfIdf$new()
tfidf_mat <- as(tfidf$fit_transform(dtm), "dgCMatrix")
rm(dtm); gc()

sec_mat   <- tfidf_mat[1:n, , drop = FALSE]
text_mat  <- tfidf_mat[(n + 1):(2*n), , drop = FALSE]
title_mat <- tfidf_mat[(2*n + 1):(3*n), , drop = FALSE]
if (ncol(sec_mat) == 0) stop("TF-IDF has 0 columns. Try increasing VOCAB_TERM_MAX.")

# -------------------------
# Vectorised cosine distance
# -------------------------
cosine_dist_vec <- function(A, B) {
  A_len <- sqrt(Matrix::rowSums(A * A))
  B_len <- sqrt(Matrix::rowSums(B * B))
  valid <- (A_len > 0) & (B_len > 0)

  out <- rep(NA_real_, nrow(A))
  if (any(valid)) {
    A_norm <- A[valid, , drop = FALSE] / A_len[valid]
    B_norm <- B[valid, , drop = FALSE] / B_len[valid]
    cos_sim <- Matrix::rowSums(A_norm * B_norm)
    out[valid] <- 1 - cos_sim
  }
  out
}

df_val$div_title_tfidf <- cosine_dist_vec(sec_mat, title_mat)  # outcome
df_val$div_text_tfidf  <- cosine_dist_vec(sec_mat, text_mat)   # predictor 1


# Jensen–Shannon divergence (normalized to [0,1])

kl_div <- function(p, q) {
  idx <- (p > 0) & (q > 0)
  sum(p[idx] * log(p[idx] / q[idx]))
}
js_div <- function(p, q) {
  m <- 0.5 * (p + q)
  0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)
}

df_val$jsd_text <- vapply(
  seq_len(n),
  function(i) {
    p <- sec_mat[i, ]
    q <- text_mat[i, ]
    sp <- sum(p); sq <- sum(q)
    if (sp == 0 || sq == 0) return(NA_real_)
    p <- p / sp
    q <- q / sq
    js_div(p, q) / log(2)
  },
  numeric(1)
)

# -------------------------
# Regressions (individual + combined)
# -------------------------
m_cos_only <- lm(div_title_tfidf ~ div_text_tfidf, data = df_val)
m_jsd_only <- lm(div_title_tfidf ~ jsd_text, data = df_val)
m_both     <- lm(div_title_tfidf ~ div_text_tfidf + jsd_text, data = df_val)

# -------------------------
# Stargazer LaTeX output
# -------------------------
stargazer(
  m_cos_only, m_jsd_only, m_both,
  type = "latex",
  title = "Validity regressions (Outcome: TF--IDF cosine divergence, section vs title)",
  column.labels = c("Cosine only", "JSD only", "Cosine + JSD"),
  dep.var.labels = "TF--IDF cosine divergence: section vs title",
  covariate.labels = c("TF--IDF cosine divergence: section vs text", "Jensen--Shannon divergence: section vs text"),
  digits = 3,
  omit.stat = c("f", "ser"),
  no.space = TRUE
)


```

---
# Rule-Based Topic Classification

A transparent rule-based classifier assigns broad policy topic labels based on keyword frequency. This approach prioritises interpretability and cross-jurisdictional consistency.

```{r}
keyword_map <- list(
  taxation    = c("tax","vat","income tax","corporate tax"),
  healthcare  = c("health","hospital","medical","insurance"),
  education   = c("school","education","university","student"),
  environment = c("environment","pollution","emission","climate"),
  criminal    = c("crime","criminal","penalty","offence"),
  immigration = c("immigr","visa","asylum","refugee"),
  labor       = c("labor","labour","employment","wage","union"),
  housing     = c("housing","rent","tenant","mortgage"),
  energy      = c("energy","electricity","oil","gas"),
  defense     = c("defense","military","armed forces"),
  finance     = c("budget","fiscal","revenue","debt"),
  technology  = c("technology","digital","software","ai"),
  social      = c("welfare","benefit","inequality"),
  justice     = c("court","justice","judicial")
)

clean_text <- function(x) {
  x |>
    tolower() |>
    str_replace_all("[^a-z0-9\\s]", " ") |>
    str_squish()
}

assign_label <- function(txt, keyword_map) {
  if (is.na(txt) || nchar(txt) == 0) return("none")
  txt <- clean_text(txt)
  scores <- sapply(keyword_map, function(kws) {
    sum(sapply(kws, function(k) str_count(txt, fixed(k))))
  })
  if (max(scores) == 0) "none" else names(scores)[which.max(scores)]
}

df_all$section_label <- vapply(df_all$section, assign_label, character(1), keyword_map)
df_all$title_label   <- vapply(df_all$title,   assign_label, character(1), keyword_map)
```

# ============================================================
# Legislative Text Analysis Pipeline
# ============================================================
# Includes:
# 1. Random sample LaTeX validation
# 2. External law parsing + CSV export
# 3. TF-IDF semantic divergence
# 4. Extreme divergence LaTeX table
# 5. Data quality diagnostics (word counts)
# ============================================================

# -----------------------------
# Libraries
# -----------------------------

library(dplyr)
library(stringr)
library(purrr)
library(readr)
library(text2vec)
library(Matrix)
library(ggplot2)
library(knitr)
library(kableExtra)

# ============================================================
# 1. Utility Functions
# ============================================================

escape_latex <- function(x) {
  str_replace_all(x, "([%&_#])", "\\\\\\1")
}

extract_first_sentences <- function(text, n = 2) {
  sentences <- str_split(text, "(?<=[.!?])\\s+")
  map_chr(sentences, ~ paste(head(.x, n), collapse = " "))
}

read_law_text <- function(path) {
  paste(readLines(path, encoding = "UTF-8", warn = FALSE),
        collapse = "\n")
}

# ============================================================
# 2. Random Sample Validation (LaTeX Export)
# ============================================================

build_sample_latex_table <- function(df, output_path,
                                     seed = 123,
                                     n_laws = 10) {

  set.seed(seed)

  sampled_titles <- df %>%
    distinct(title) %>%
    slice_sample(n = n_laws) %>%
    pull(title)

  df_prepped <- df %>%
    filter(title %in% sampled_titles) %>%
    mutate(
      section_short = extract_first_sentences(section, 2),
      divergence_vs_title = round(divergence_vs_title, 2),
      hiddenness_textual  = round(hiddenness_textual, 2)
    ) %>%
    arrange(title, divergence_vs_title)

  latex_groups <- df_prepped %>%
    group_by(title) %>%
    summarise(
      tex = paste0(
        "\\addlinespace\n",
        "\\multicolumn{3}{l}{\\textbf{",
        escape_latex(unique(title)),
        "} (", unique(text_sentences), " sentences)} \\\\\n",
        "\\addlinespace\n",
        paste0(
          escape_latex(section_short),
          " & ",
          divergence_vs_title,
          " & ",
          hiddenness_textual,
          " \\\\",
          collapse = "\n"
        ),
        "\n"
      ),
      .groups = "drop"
    )

  latex_table <- paste0(
"\\begin{longtable}{
>{\\RaggedRight\\arraybackslash}p{9cm}
>{\\centering\\arraybackslash}p{1.4cm}
>{\\centering\\arraybackslash}p{1.4cm}
}
\\toprule
Section (first 2 sentences) & Divergence & Hiddenness \\\\
\\midrule
\\endfirsthead
\\toprule
Section (first 2 sentences) & Divergence & Hiddenness \\\\
\\midrule
\\endhead
",
paste(latex_groups$tex, collapse = "\n"),
"\\bottomrule
\\end{longtable}"
  )

  writeLines(latex_table, output_path)
}

# ============================================================
# 3. External Validation: Law Parsing
# ============================================================

parse_us_law <- function(text, law_title) {

  section_ids <- str_extract_all(
    text,
    regex("\\nSEC\\.\\s+[0-9A-Za-z\\.\\-]+", ignore_case = TRUE)
  )[[1]]

  sections <- str_split(
    text,
    regex("\\nSEC\\.\\s+[0-9A-Za-z\\.\\-]+", ignore_case = TRUE)
  )[[1]]

  tibble(
    law_title   = law_title,
    section_id  = str_trim(section_ids),
    section_full = str_trim(sections[-1])
  )
}

parse_gdpr <- function(text, law_title) {

  article_ids <- str_extract_all(
    text,
    regex("\\nArticle\\s+[0-9]+", ignore_case = TRUE)
  )[[1]]

  articles <- str_split(
    text,
    regex("\\nArticle\\s+[0-9]+", ignore_case = TRUE)
  )[[1]]

  tibble(
    law_title   = law_title,
    section_id  = str_trim(article_ids),
    section_full = str_trim(articles[-1])
  )
}

parse_laws <- function(law_files) {

  law_files %>%
    mutate(
      text = map(file_path, read_law_text),
      parsed = pmap(
        list(text, law_title),
        function(text, law_title) {
          if (str_detect(law_title, "General Data Protection Regulation")) {
            parse_gdpr(text, law_title)
          } else {
            parse_us_law(text, law_title)
          }
        }
      )
    ) %>%
    pull(parsed) %>%
    bind_rows() %>%
    mutate(section_full = str_squish(section_full)) %>%
    filter(section_full != "")
}

# ============================================================
# 4. Semantic Divergence (TF-IDF Cosine Distance)
# ============================================================

compute_semantic_divergence <- function(df,
                                        text_col = "section_full",
                                        title_col = "law_title",
                                        vocab_max = 3000) {

  corpus <- c(df[[text_col]], df[[title_col]])
  it <- itoken(corpus, tokenizer = word_tokenizer, progressbar = TRUE)

  vocab <- create_vocabulary(it, vocab_term_max = vocab_max)
  vectorizer <- vocab_vectorizer(vocab)
  dtm <- create_dtm(it, vectorizer)

  tfidf <- TfIdf$new()
  tfidf_mat <- as(tfidf$fit_transform(dtm), "dgCMatrix")

  n <- nrow(df)

  sec_mat   <- tfidf_mat[1:n, ]
  title_mat <- tfidf_mat[(n + 1):(2 * n), ]

  sec_len   <- sqrt(rowSums(sec_mat * sec_mat))
  title_len <- sqrt(rowSums(title_mat * title_mat))

  valid <- (sec_len > 0) & (title_len > 0)

  divergence <- rep(NA_real_, n)

  divergence[valid] <-
    1 - rowSums(
      (sec_mat[valid, ] / sec_len[valid]) *
      (title_mat[valid, ] / title_len[valid])
    )

  divergence
}

# ============================================================
# 5. Extreme Divergence LaTeX Table
# ============================================================

build_extreme_table <- function(df, output_path) {

  df %>%
    group_by(law_title) %>%
    arrange(semantic_divergence, .by_group = TRUE) %>%
    mutate(extreme = case_when(
      row_number() <= 5 ~ "Least divergent",
      row_number() > n() - 5 ~ "Most divergent",
      TRUE ~ NA_character_
    )) %>%
    filter(!is.na(extreme)) %>%
    ungroup() %>%
    select(law_title, extreme, section_full, semantic_divergence) %>%
    kable(
      format   = "latex",
      booktabs = TRUE,
      escape   = TRUE,
      caption  = "Most and least semantically divergent sections by law"
    ) %>%
    kable_styling(latex_options = "hold_position", font_size = 9) %>%
    writeLines(output_path)
}

# ============================================================
# 6. Data Quality: Word Count Diagnostics
# ============================================================

compute_word_length <- function(df) {

  df %>%
    mutate(
      word_length = case_when(
        country == "France" ~ str_count(text, "\\S+"),
        TRUE ~ str_count(section, "\\S+")
      )
    ) %>%
    filter(!is.na(word_length), word_length >= 10)
}

plot_word_density <- function(df, output_file, log_scale = FALSE) {

  p <- ggplot(df, aes(x = word_length, fill = country)) +
    geom_density(alpha = 0.4) +
    theme_minimal()

  if (log_scale) {
    p <- p + scale_x_log10()
  } else {
    p <- p + coord_cartesian(xlim = c(10, 1500))
  }

  ggsave(output_file, p, width = 8, height = 6, dpi = 300)
}

# Remove text an section variable to make the file less computationally heavy

```{r}
df_all <- df_all %>%
  select(-text, -section)
```

#Save the dataset
```{r}
Output
write.csv( 
  df_all,
  file.path(data_dir, "Total_with_metrics_classified.csv"),
  row.names = FALSE
)
```







